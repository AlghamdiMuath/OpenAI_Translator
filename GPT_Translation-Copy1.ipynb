{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187cc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0246fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 235\n"
     ]
    }
   ],
   "source": [
    "def wrap_text(text, width=120):\n",
    "    wrapper = textwrap.TextWrapper(width=width)\n",
    "    return wrapper.fill(text=text)\n",
    "\n",
    "def tokenize_file(file_path,model):\n",
    "    with open(file_path, 'r', encoding=\"utf8\") as file:\n",
    "        text = file.read()\n",
    "    encoding = tiktoken.get_encoding(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    return tokens\n",
    "\n",
    "def split_tokens_into_chunks(tokens, chunk_size):\n",
    "    num_chunks = (len(tokens) + chunk_size - 1) // chunk_size\n",
    "    chunks = [tokens[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks)]\n",
    "    return chunks\n",
    "\n",
